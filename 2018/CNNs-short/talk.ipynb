{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning, what a hype\n",
    "\n",
    "### Me: Adrin [adrin.info](http://adrin.info)\n",
    "\n",
    "\n",
    "### Ancud IT-Beratung [ancud.de](https://ancud.de)\n",
    "![ancud](figs/ancud.png)\n",
    "\n",
    "\n",
    "### This talk: [github.com/adrinjalali/2017-05-talk-dl](https://github.com/adrinjalali/2017-05-talk-dl)\n",
    "#### Requirements: python3, ipython, notebook (jupyter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "### A single neuron\n",
    "\n",
    "![spiking neural network](http://lis2.epfl.ch/CompletedResearchProjects/EvolutionOfAdaptiveSpikingCircuits/images/neuron.jpg)\n",
    "\n",
    "![spiking system](http://lis2.epfl.ch/CompletedResearchProjects/EvolutionOfAdaptiveSpikingCircuits/images/spiking.jpg)\n",
    "\n",
    "### Artificial neuron\n",
    "[Source](http://natureofcode.com/book/chapter-10-neural-networks/)\n",
    "\n",
    "![](http://natureofcode.com/book/imgs/chapter10/ch10_05.png)\n",
    "\n",
    "#### Add bias\n",
    "![](http://natureofcode.com/book/imgs/chapter10/ch10_06.png)\n",
    "\n",
    "\n",
    "#### Feed the data\n",
    "![](http://natureofcode.com/book/imgs/chapter10/ch10_07.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo [here](http://natureofcode.com/book/chapter-10-neural-networks/)\n",
    "\n",
    "### Linearly separable, and not\n",
    "![](http://natureofcode.com/book/imgs/chapter10/ch10_11.png)\n",
    "\n",
    "\n",
    "#### Logic example:\n",
    "![](http://natureofcode.com/book/imgs/chapter10/ch10_12.png)\n",
    "![](http://natureofcode.com/book/imgs/chapter10/ch10_13.png)\n",
    "\n",
    "#### Multilayer perceptron\n",
    "![](http://natureofcode.com/book/imgs/chapter10/ch10_14.png)\n",
    "\n",
    "### Activation Functions: [wiki](https://en.wikipedia.org/wiki/Activation_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures\n",
    "\n",
    "### Feedforward\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/560px-Artificial_neural_network.svg.png)\n",
    "\n",
    "\n",
    "### Recurrent\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/7/79/Recurrent_ann_dependency_graph.png)\n",
    "\n",
    "\n",
    "#### Elman SRNN\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/8/8f/Elman_srnn.png)\n",
    "\n",
    "### Unsupervised, eg. SOM\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Somtraining.svg/1000px-Somtraining.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New developments\n",
    "\n",
    "### General-purpose computing on graphics processing units (GPGPU)\n",
    "\n",
    "#### GPU vs CPU\n",
    "![](http://www.frontiersin.org/files/Articles/70265/fgene-04-00266-HTML/image_m/fgene-04-00266-g001.jpg)\n",
    "\n",
    "\n",
    "#### 2005\n",
    "![](figs/gpgpu.png)\n",
    "\n",
    "\n",
    "### Better algorithms\n",
    "\n",
    "#### 2011\n",
    "![](figs/2011-conv-mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Convolutional Neural Networks (CNN)\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)\n",
    "\n",
    "### Weight Sharing, Convolution\n",
    "\n",
    "### Max Pooling\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png)\n",
    "\n",
    "### Dropout, {L1, L2} regularization, artificial data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "![](http://andrea.burattin.net/public-files/stuff/handwritten-digit-recognition/example_mnist.gif)\n",
    "\n",
    "![](figs/mnist-perfs.png)\n",
    "\n",
    "### Based on keras examples, specifically [this one](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in ./.venv/lib/python3.6/site-packages\n",
      "Requirement already satisfied: tensorflow in ./.venv/lib/python3.6/site-packages\n",
      "Requirement already satisfied: theano in ./.venv/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in ./.venv/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.11.0 in ./.venv/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in ./.venv/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in ./.venv/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: scipy>=0.14 in ./.venv/lib/python3.6/site-packages (from theano->keras)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.6/site-packages (from protobuf>=3.2.0->tensorflow)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in ./.venv/lib/python3.6/site-packages (from setuptools->protobuf>=3.2.0->tensorflow)\n",
      "Requirement already satisfied: packaging>=16.8 in ./.venv/lib/python3.6/site-packages (from setuptools->protobuf>=3.2.0->tensorflow)\n",
      "Requirement already satisfied: pyparsing in ./.venv/lib/python3.6/site-packages (from packaging>=16.8->setuptools->protobuf>=3.2.0->tensorflow)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 190s - loss: 0.3282 - acc: 0.9009 - val_loss: 0.0752 - val_acc: 0.9771\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 191s - loss: 0.1101 - acc: 0.9669 - val_loss: 0.0523 - val_acc: 0.9836\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 196s - loss: 0.0848 - acc: 0.9752 - val_loss: 0.0429 - val_acc: 0.9869\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 195s - loss: 0.0708 - acc: 0.9790 - val_loss: 0.0381 - val_acc: 0.9871\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 191s - loss: 0.0625 - acc: 0.9812 - val_loss: 0.0326 - val_acc: 0.9891\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 184s - loss: 0.0545 - acc: 0.9839 - val_loss: 0.0330 - val_acc: 0.9890\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 187s - loss: 0.0507 - acc: 0.9844 - val_loss: 0.0307 - val_acc: 0.9894\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 191s - loss: 0.0451 - acc: 0.9867 - val_loss: 0.0291 - val_acc: 0.9898\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 189s - loss: 0.0434 - acc: 0.9868 - val_loss: 0.0297 - val_acc: 0.9907\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 184s - loss: 0.0402 - acc: 0.9874 - val_loss: 0.0296 - val_acc: 0.9898\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 183s - loss: 0.0383 - acc: 0.9884 - val_loss: 0.0277 - val_acc: 0.9902\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 181s - loss: 0.0363 - acc: 0.9890 - val_loss: 0.0262 - val_acc: 0.9911\n",
      "Test loss: 0.0261879921081\n",
      "Test accuracy: 0.9911\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good old scikit-learn & linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in ./.venv/lib/python3.6/site-packages\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.6/site-packages (from sklearn)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrin/Projects/Conferences/nuernberg-deellearning/.venv/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('variance filter', VarianceThreshold(threshold=0.01)), ('standard_scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=2000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('variance filter', VarianceThreshold(threshold=0.01)),\n",
    "    ('standard_scale', StandardScaler()),\n",
    "    ('estimator', Lasso(alpha=0.1, max_iter=2000)),\n",
    "])\n",
    "\n",
    "pipeline.fit(x_train.reshape(60000, -1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47789595238095106"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "label_ranking_average_precision_score(y_test, pipeline.predict(x_test.reshape(len(x_test), -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99523333333333353"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ranking_average_precision_score(y_test, model.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "![](figs/normal.jpg)\n",
    "\n",
    "### Adding classes\n",
    "\n",
    "![](figs/added-nodes.jpg)\n",
    "\n",
    "### Dimentionality reduction / Transfer learning\n",
    "\n",
    "![](figs/dimentionality-reduction.jpg)\n",
    "\n",
    "![](figs/dimentionality-reduction-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remarks\n",
    "\n",
    " - Usecases with not enough data\n",
    " - Usecases with many small models\n",
    " - Gain on performance vs. cost\n",
    " - Network architecture & hyperparameters\n",
    " - Deployment\n",
    "   - Cleanup\n",
    "   - Batching\n",
    "   - Serving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
