{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to write a scikit-learn compatible estimator\n",
    "\n",
    "## Adrin Jalali\n",
    "### @adrinjalali, HuggingFace, scikit-learn, PyData Berlin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to scikit-learn\n",
    "\n",
    "A machine learning library implementing statistical machine learning methods.\n",
    "\n",
    "__Important components in the API:__\n",
    "\n",
    "- estimators (transformers, classifiers, regressors, clustering algorithms)\n",
    "- scorers\n",
    "- meta-estimators\n",
    "    - pipeline\n",
    "    - grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# A common workflow includes a pipeline once the data is loaded.\n",
    "# We usually preprocess the data and prepare it for the\n",
    "# final classifier or regressor.\n",
    "# We call each step an \"estimator\", the preprocessing steps which\n",
    "# augment the data are \"transformers\", and the final step is a\n",
    "# classifier or a regressor.\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "# Each step can be tuned with many hyper-parameters, and we want to\n",
    "# find the best hyper-parameter set for the given dataset.\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "}\n",
    "\n",
    "# find the best parameters for both the feature extraction and the\n",
    "# classifier, we use a grid search.\n",
    "grid_search = GridSearchCV(pipeline, parameters)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated pipeline!\n",
    "See the full example [here](https://scikit-learn.org/dev/auto_examples/compose/plot_column_transformer.html).\n",
    "```python\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        # Extract subject & body\n",
    "        (\"subjectbody\", subject_body_transformer),\n",
    "        # Use ColumnTransformer to combine the subject and body features\n",
    "        (\n",
    "            \"union\",\n",
    "            ColumnTransformer(\n",
    "                [\n",
    "                    # bag-of-words for subject (col 0)\n",
    "                    (\"subject\", TfidfVectorizer(min_df=50), 0),\n",
    "                    # bag-of-words with decomposition for body (col 1)\n",
    "                    (\n",
    "                        \"body_bow\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\"tfidf\", TfidfVectorizer()),\n",
    "                                (\"best\", TruncatedSVD(n_components=50)),\n",
    "                            ]\n",
    "                        ),\n",
    "                        1,\n",
    "                    ),\n",
    "                    # Pipeline for pulling text stats from post's body\n",
    "                    (\n",
    "                        \"body_stats\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\n",
    "                                    \"stats\",\n",
    "                                    text_stats_transformer,\n",
    "                                ),  # returns a list of dicts\n",
    "                                (\n",
    "                                    \"vect\",\n",
    "                                    DictVectorizer(),\n",
    "                                ),  # list of dicts -> feature matrix\n",
    "                            ]\n",
    "                        ),\n",
    "                        1,\n",
    "                    ),\n",
    "                ],\n",
    "                # weight above ColumnTransformer features\n",
    "                transformer_weights={\n",
    "                    \"subject\": 0.8,\n",
    "                    \"body_bow\": 0.5,\n",
    "                    \"body_stats\": 1.0,\n",
    "                },\n",
    "            ),\n",
    "        ),\n",
    "        # Use a SVC classifier on the combined features\n",
    "        (\"svc\", LinearSVC(dual=False)),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why a custom estimator?\n",
    "\n",
    "- scikit-learn doesn't include all algorithms, and it has a very high bar for including one. You can test your new or modified algorithm as a custom estimator.\n",
    "- The library does not include methods which are appropriate only for a small set o use-cases, and if you happen to work on one of those problems, you might need to develop your own estimator to tackle the specific issues you have.\n",
    "- You may want to add some steps before or after running each step, in which case you can write a meta-estimator wrapping around the steps you usually would have in a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic API\n",
    "\n",
    "- `fit (X, y[, sample_weight], **kwargs)`\n",
    "- `predict(X)` (`predict_proba` and `decision_function`)\n",
    "- `transform(X)`\n",
    "- `score(X, y[, sample_weight])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MyClassifier(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        self._targets = np.unique(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.asarray([self._targets[0]] * len(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "check_estimator(MyClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above check runs all estimator checks against your estimator. You can iteratively fix issues with your estimator until it passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "def check_is_fitted(estimator, attributes=None, msg=None, all_or_any=all):\n",
    "    \"\"\"Perform is_fitted validation for estimator.\n",
    "\n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "def check_array(\n",
    "    array,\n",
    "    accept_sparse=False,\n",
    "    *,\n",
    "    accept_large_sparse=True,\n",
    "    dtype=\"numeric\",\n",
    "    order=None,\n",
    "    copy=False,\n",
    "    force_all_finite=True,\n",
    "    ensure_2d=True,\n",
    "    allow_nd=False,\n",
    "    ensure_min_samples=1,\n",
    "    ensure_min_features=1,\n",
    "    estimator=None,\n",
    "    input_name=\"\",\n",
    "):\n",
    "    \"\"\"Input validation on an array, list, sparse matrix or similar.\n",
    "\n",
    "    By default, the input is checked to be a non-empty 2D array containing\n",
    "    only finite values. If the dtype of the array is object, attempt\n",
    "    converting to float, raising on failure.\n",
    "    \n",
    "    ...\n",
    "    \"\"\"\n",
    "    \n",
    "def check_X_y(\n",
    "    X,\n",
    "    y,\n",
    "    accept_sparse=False,\n",
    "    *,\n",
    "    accept_large_sparse=True,\n",
    "    dtype=\"numeric\",\n",
    "    order=None,\n",
    "    copy=False,\n",
    "    force_all_finite=True,\n",
    "    ensure_2d=True,\n",
    "    allow_nd=False,\n",
    "    multi_output=False,\n",
    "    ensure_min_samples=1,\n",
    "    ensure_min_features=1,\n",
    "    y_numeric=False,\n",
    "    estimator=None,\n",
    "):\n",
    "    \"\"\"Input validation for standard estimators.\n",
    "\n",
    "    Checks X and y for consistent length, enforces X to be 2D and y 1D. By\n",
    "    default, X is checked to be non-empty and containing only finite values.\n",
    "    Standard input checks are also applied to y, such as checking that y\n",
    "    does not have np.nan or np.inf targets. For multi-label y, set\n",
    "    multi_output=True to allow 2D and sparse y. If the dtype of X is\n",
    "    object, attempt converting to float, raising on failure.\n",
    "    \n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def check_classification_targets(y):\n",
    "    \"\"\"Ensure that target y is of a non-regression type.\n",
    "\n",
    "    Only the following target types (as defined in type_of_target) are allowed:\n",
    "        'binary', 'multiclass', 'multiclass-multioutput',\n",
    "        'multilabel-indicator', 'multilabel-sequences'\n",
    "    ...\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Common Tests on Our Classifier with pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import parametrize_with_checks\n",
    "\n",
    "@parametrize_with_checks([MyClassifier()])\n",
    "def test_sklearn_compatible_estimator(estimator, check):\n",
    "    check(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class MyTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def __sklearn_is_fitted__(self):\n",
    "        return True\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "check_estimator(MyTransformer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer in a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "est = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ").fit(X_train, y_train)\n",
    "est[:-1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = make_pipeline(\n",
    "    MyTransformer(),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ").fit(X_train, y_train)\n",
    "est[:-1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimedEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, MetaEstimatorMixin, clone\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import time\n",
    "\n",
    "\n",
    "class TimedEstimator(MetaEstimatorMixin, BaseEstimator):\n",
    "    def __init__(self, estimator, verbose=0):\n",
    "        self.estimator = estimator\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        start = time.perf_counter()\n",
    "        self.estimator_ = clone(self.estimator).fit(X, y, **fit_params)\n",
    "        end = time.perf_counter()\n",
    "        self.time_ = end - start\n",
    "        if self.verbose > 0:\n",
    "            print(self.time_)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "check_estimator(TimedEstimator(estimator=LogisticRegression()))\n",
    "check_estimator(TimedEstimator(estimator=LinearRegression()))\n",
    "check_estimator(TimedEstimator(estimator=StandardScaler()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimedEstimator for and in a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = TimedEstimator(\n",
    "    make_pipeline(\n",
    "        MyTransformer(),\n",
    "        StandardScaler(),\n",
    "        TimedEstimator(LogisticRegression())\n",
    "    )\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est[-1].time_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "est = make_pipeline(\n",
    "        MyTransformer(),\n",
    "        StandardScaler(),\n",
    "        LogisticRegression()\n",
    ")\n",
    "\n",
    "grid = {\n",
    "    \"logisticregression__C\": [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=est, param_grid=grid)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "est = TimedEstimator(\n",
    "    make_pipeline(\n",
    "        MyTransformer(),\n",
    "        StandardScaler(),\n",
    "        TimedEstimator(LogisticRegression())\n",
    "    )\n",
    ")\n",
    "\n",
    "grid = {\n",
    "    \"estimator\": [\n",
    "        make_pipeline(\n",
    "            MyTransformer(),\n",
    "            StandardScaler(),\n",
    "            TimedEstimator(LogisticRegression(C=C))\n",
    "        )\n",
    "        for C in [0.01, 0.1, 1, 10]\n",
    "    ]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=est, param_grid=grid)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "est = TimedEstimator(\n",
    "    make_pipeline(\n",
    "        MyTransformer(),\n",
    "        StandardScaler(),\n",
    "        TimedEstimator(LogisticRegression())\n",
    "    )\n",
    ")\n",
    "\n",
    "grid = {\n",
    "    \"logisticregression__C\": [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=est, param_grid=grid)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventions\n",
    "\n",
    "- `fit` should only get sample aligned data in `fit_params`\n",
    "    - everything else should go through `__init__`\n",
    "- `__init__` doesn't set anything other than the parameters passed to it\n",
    "- `obj.attr` is set through `__init__` and `set_params`\n",
    "- `obj.attr_` is set during fit and counts as public API\n",
    "- `obj._attr` is private\n",
    "- `n_features_in_`, `feature_names_in_`, and `get_feature_names_out()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Tags\n",
    "\n",
    "``` python\n",
    "_DEFAULT_TAGS = {\n",
    "    \"non_deterministic\": False,\n",
    "    \"requires_positive_X\": False,\n",
    "    \"requires_positive_y\": False,\n",
    "    \"X_types\": [\"2darray\"],\n",
    "    \"poor_score\": False,\n",
    "    \"no_validation\": False,\n",
    "    \"multioutput\": False,\n",
    "    \"allow_nan\": False,\n",
    "    \"stateless\": False,\n",
    "    \"multilabel\": False,\n",
    "    \"_skip_test\": False,\n",
    "    \"_xfail_checks\": False,\n",
    "    \"multioutput_only\": False,\n",
    "    \"binary_only\": False,\n",
    "    \"requires_fit\": True,\n",
    "    \"preserves_dtype\": [np.float64],\n",
    "    \"requires_y\": False,\n",
    "    \"pairwise\": False,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upcoming Features\n",
    "\n",
    "- passing feature names along the pipeline during `fit`: [SLEP#18](https://github.com/scikit-learn/enhancement_proposals/pull/68)\n",
    "- sample/feature/data properties through `get_metadata_routing()` and `set_{method}_request()` [SLEP#6](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep006/proposal.html), [#22893](https://github.com/scikit-learn/scikit-learn/issues/22893)\n",
    "- standardized parameter validation through `validate_params` [#23462](https://github.com/scikit-learn/scikit-learn/issues/23462)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Details/Further Reading\n",
    "\n",
    "- [https://scikit-learn.org/dev/developers/develop.html](https://scikit-learn.org/dev/developers/develop.html)\n",
    "- `sklearn/base.py`\n",
    "- `sklearn/metaestimators.py`\n",
    "- `sklearn/utils/metaestimators.py`\n",
    "- `sklearn/utils/validation.py`\n",
    "- `check_random_state`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "### Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
